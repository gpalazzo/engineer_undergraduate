{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from keras) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: h5py in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: ann_visualizer in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (2.5)\n",
      "Requirement already satisfied: graphviz in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (0.17)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp38-cp38-macosx_10_11_x86_64.whl (195.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 195.7 MB 83 kB/s s eta 0:00:0114     |██████████▍                     | 63.4 MB 4.9 MB/s eta 0:00:28     |█████████████▍                  | 81.6 MB 30.2 MB/s eta 0:00:04     |██████████████▍                 | 87.9 MB 30.2 MB/s eta 0:00:04     |██████████████████████▉         | 139.5 MB 90.8 MB/s eta 0:00:01     |███████████████████████▎        | 142.6 MB 90.8 MB/s eta 0:00:01██████████████▏       | 147.6 MB 5.7 MB/s eta 0:00:09     |████████████████████████▎       | 148.6 MB 5.7 MB/s eta 0:00:09     |█████████████████████████▋      | 156.3 MB 5.7 MB/s eta 0:00:07\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 20.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.6 MB 18.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 19.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 33.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 30.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp38-cp38-macosx_10_10_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 22.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.3-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 17.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 8.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.5-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (49.2.0.post20200714)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.34.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 21.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 30.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/guilhermepalazzo/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: wrapt, termcolor\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl size=32732 sha256=6458829717958ba215a753dcc8bad4858370724c462e105effe0e63cc79bc7d0\n",
      "  Stored in directory: /Users/guilhermepalazzo/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=b3753a0d8d73aeef48ed26c69531439409ef200a162e25f019110170f83ea9b6\n",
      "  Stored in directory: /Users/guilhermepalazzo/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built wrapt termcolor\n",
      "Installing collected packages: google-pasta, flatbuffers, typing-extensions, numpy, h5py, wrapt, absl-py, keras-preprocessing, grpcio, wheel, tensorboard-plugin-wit, markdown, oauthlib, requests-oauthlib, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-auth-oauthlib, protobuf, tensorboard-data-server, tensorboard, termcolor, astunparse, gast, opt-einsum, tensorflow-estimator, keras-nightly, tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Uninstalling wheel-0.34.2:\n",
      "      Successfully uninstalled wheel-0.34.2\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.34.0 google-auth-oauthlib-0.4.5 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wheel-0.36.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras\n",
    "!pip3 install ann_visualizer\n",
    "!pip3 install graphviz\n",
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir = \".\"\n",
    "file_name = 'German_Dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqL92tyIg4rk"
   },
   "outputs": [],
   "source": [
    "_dict = {}\n",
    "\n",
    "funcao_perda = \"binary_crossentropy\" #@param [\"binary_crossentropy\", \"mean_squared_error\"] {type: \"string\", allow-input: false}\n",
    "funcao_ativacao_cam_oculta = \"relu\" #@param [\"softmax\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"relu\", \"tanh\", \"sigmoid\", \"hard_sigmoid\", \"exponential\", \"linear\"] {type: \"string\", allow-input: false}\n",
    "funcao_ativacao_cam_saida = \"sigmoid\" #@param [\"softmax\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"relu\", \"tanh\", \"sigmoid\", \"hard_sigmoid\", \"exponential\", \"linear\"] {type: \"string\", allow-input: false}\n",
    "epocas = 100 #@param {type: \"slider\", min: 1, max: 300, step: 1, allow-input: true}\n",
    "otimizador = \"sgd\" #@param [\"sgd\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"Adam\", \"Adamax\", \"Nadam\"] {type: \"string\", allow-input: false}\n",
    "tam_lote = 20 #@param {type: \"slider\", min: 1, max: 128, step: 1, allow-input:true}\n",
    "qtd_neuro_cam_oculta = 60 #@param {type: \"slider\", min: 1, max: 125, step: 1, allow-input:true}\n",
    "porcentagem_treinamento = 70 #@param {type: \"slider\", min: 1, max: 100, step: 1, allow-input:true}\n",
    "\n",
    "_dict['funcao_perda'] = funcao_perda\n",
    "_dict['funcao_ativacao_cam_oculta'] = funcao_ativacao_cam_oculta\n",
    "_dict['funcao_ativacao_cam_saida'] = funcao_ativacao_cam_saida\n",
    "_dict['epocas'] = epocas\n",
    "_dict['otimizador'] = otimizador\n",
    "_dict['tam_lote'] = tam_lote\n",
    "_dict['qtd_neuro_cam_oculta'] = qtd_neuro_cam_oculta\n",
    "_dict['prc_train'] = porcentagem_treinamento\n",
    "\n",
    "dados_str = \"{qtd_neuro_cam_oculta}, {funcao_ativacao_cam_oculta}, {funcao_ativacao_cam_saida}, {funcao_perda}, {otimizador}, {epocas}, {tam_lote}, {prc_train}\".format(**_dict)\n",
    "dados = dados_str.split(\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13234,
     "status": "ok",
     "timestamp": 1542477808349,
     "user": {
      "displayName": "Guilherme Palazzo",
      "photoUrl": "https://lh3.googleusercontent.com/-C7q-Ik-g3xY/AAAAAAAAAAI/AAAAAAAAABI/dvtwO6gXTk4/s64/photo.jpg",
      "userId": "06775809968580977377"
     },
     "user_tz": 120
    },
    "id": "43ge5_uW46C_",
    "outputId": "f9c3368d-6d50-42cc-db1e-c2b89ccc79f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Acurácia média do modelo na iteração 1: 0.00%\n",
      "22/22 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Acurácia média do modelo na iteração 2: 0.00%\n",
      "22/22 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Acurácia média do modelo na iteração 3: 0.00%\n",
      "Média geral das 3 iterações: 0.00%\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(_dir + \"/\" + file_name, sep=\",\")\n",
    "\n",
    "df = dataset.copy()\n",
    "\n",
    "df['credit_history'] = df['credit_history'].map({\"A30\": 5, \n",
    "                                                 \"A31\": 4,\n",
    "                                                 \"A32\": 3,\n",
    "                                                 \"A33\": 2,\n",
    "                                                 \"A34\": 1})\n",
    "\n",
    "df['present_employment_since'] = df['present_employment_since'].map({\"A75\": 5, \n",
    "                                                                     \"A74\": 4,\n",
    "                                                                     \"A73\": 3,\n",
    "                                                                     \"A72\": 2,\n",
    "                                                                     \"A71\": 1})\n",
    "\n",
    "df['housing'] = df['housing'].map({\"A152\": 3,\n",
    "                                   \"A151\": 2,\n",
    "                                   \"A153\": 1})\n",
    "\n",
    "df['other_debtors_guarantors'] = df['other_debtors_guarantors'].map({\"A103\": 3,\n",
    "                                                                     \"A102\": 2,\n",
    "                                                                     \"A101\": 1})\n",
    "\n",
    "df['property'] = df['property'].map({\"A121\": 4, \n",
    "                                     \"A122\": 3, \n",
    "                                     \"A123\": 2, \n",
    "                                     \"A124\": 1})\n",
    "\n",
    "df['job'] = df['job'].map({\"A174\": 4, \n",
    "                           \"A173\": 3, \n",
    "                           \"A172\": 2, \n",
    "                           \"A171\": 1})\n",
    "\n",
    "\n",
    "#30% das instâncias são maus pagadores\n",
    "#a proporção de treinamento e teste será 700:300\n",
    "#ou seja, 700 instâncias para treinamento, então preciso de 35% de cada instância para treinamento\n",
    "\n",
    "#ou seja, preciso de 10,5% de maus pagadores do total para compor o treinamento = \n",
    "#e 24,5% de bons pagadores do total = \n",
    "\n",
    "\n",
    "#1: Good\n",
    "#2: Bad\n",
    "\n",
    "df_dummies = pd.get_dummies(df)\n",
    "#print(df_dummies)\n",
    "\n",
    "\n",
    "cols = list(df_dummies.columns.values)\n",
    "cols.pop(cols.index('final_classification'))\n",
    "df_dummies = df_dummies[cols+['final_classification']]\n",
    "\n",
    "qtd_bad = 0\n",
    "qtd_good = 0\n",
    "train_dataset = []\n",
    "expec_ans_train = []\n",
    "index_todrop = []\n",
    "\n",
    "for i in range(len(df_dummies.credit_amount)):\n",
    "    \n",
    "    if (qtd_good + qtd_bad) == (len(df_dummies.credit_amount) * 0.01 * (int(dados[7]))):\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        if (df_dummies.final_classification[i] == 1):\n",
    "            \n",
    "            if (qtd_good == round((int(dados[7]) * 0.01 * 0.7 * len(df_dummies.credit_amount)), 0)):\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                qtd_good += 1\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if (qtd_bad == round((int(dados[7]) * 0.01 * 0.3 * len(df_dummies.credit_amount)), 0)):\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                qtd_bad += 1\n",
    "        \n",
    "        data = list(df_dummies.iloc[i])\n",
    "        index_todrop.append(i)\n",
    "        expec_ans_train.append(data.pop())\n",
    "        train_dataset.append(data)\n",
    "\n",
    "\n",
    "df_dummies.drop(df_dummies.index[index_todrop], inplace=True)\n",
    "\n",
    "expec_ans_test = list(df_dummies['final_classification'])\n",
    "\n",
    "df_dummies.drop('final_classification', axis=1, inplace=True)\n",
    "n_lines = df_dummies.shape[0]\n",
    "test_dataset = []\n",
    "media_3_iter = []\n",
    "\n",
    "for i in range(len(expec_ans_test)):\n",
    "    test_dataset.append(list(df_dummies.iloc[i]))\n",
    "\n",
    "for j in range(3):\n",
    "\n",
    "    X_train = np.array(train_dataset)\n",
    "    Y_train = np.array(expec_ans_train)\n",
    "    X_test = np.array(test_dataset)\n",
    "    Y_test = np.array(expec_ans_test)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(len(df_dummies.columns.values), input_dim=len(df_dummies.columns.values), activation=dados[1]))\n",
    "    model.add(Dense(int(dados[0]), activation=dados[1]))\n",
    "    model.add(Dense(int(dados[0]), activation=dados[1]))\n",
    "    model.add(Dense(1, activation=dados[2]))\n",
    "\n",
    "    model.compile(loss=dados[3], optimizer=dados[4], metrics=['accuracy'])\n",
    "        \n",
    "    model.fit(X_train, Y_train, validation_split=0.25, epochs=int(dados[5]), batch_size=int(dados[6]), verbose=False)\n",
    "\n",
    "    #answers = model.predict(X_test, batch_size=int(dados[6]))\n",
    "\n",
    "    scores = model.evaluate(X_train, Y_train)\n",
    "    media_3_iter.append(scores[1])\n",
    "    print(\"Acurácia média do modelo na iteração {}: {:.2f}%\".format(j+1, scores[1]*100))\n",
    "\n",
    "\n",
    "    #print(\"Todo o processo demorou {} segundos\".format(time.time() - start_time))\n",
    "print(\"Média geral das 3 iterações: {:.2f}%\".format(np.mean(media_3_iter)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of TCC_UNAERP.ipynb",
   "provenance": [
    {
     "file_id": "1Hk4i1CBzQWbFhBIiIaSNJKc7MLip_9qE",
     "timestamp": 1542477445753
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
